{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you should try to implement some of the techniques discussed in the lecture.\n",
    "Here is a list of reasonable tasks.\n",
    "\n",
    "Must implement:\n",
    " * Log-loss\n",
    " \n",
    "Easy:\n",
    " * L1 and L2 regularization (you can choose one)\n",
    " * momentum, Nesterov's momentum (you can choose one)\n",
    "\n",
    "Medium difficulty:\n",
    " * Adagrad, RMSProp (you can shoose one) - not much harder than momentum, really\n",
    " * dropout\n",
    "\n",
    "Hard (and time-consuming):\n",
    " * batch-normalization\n",
    "\n",
    "Try to test your network to see if these changes improve accuracy. They improve accuracy much more if you increase the layer size, and if you add more layers, say 1 or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    # Derivative of the sigmoid\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.101"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1 complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2 complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 3 complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 4 complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 5 complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 6 complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 7 complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 8 complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 9 complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 10: 0.1135\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEDCAYAAAA7jc+ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc56f18ddd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# apply regularization\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    # Derivative of the sigmoid\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class Layer(object):\n",
    "    def descent(self, lr, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def get_weigths(self):\n",
    "        return []\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return []\n",
    "    \n",
    "class Linear(Layer):\n",
    "    def __init__(self, ins, outs):\n",
    "        self.w = np.random.normal(scale=(1.0  / (ins + outs)), size=[ins, outs])\n",
    "        self.b = np.random.normal(scale=0.1, size=[outs])\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input.shape = (batch_size, inp_width)\n",
    "        # w.shape = (inp_width, outp_width)\n",
    "        # b.shape = (outp_width)\n",
    "        return np.dot(input, self.w) + self.b\n",
    "\n",
    "    def backward(self, orig_input, orig_output, output_gradient):\n",
    "        # gp ~ bs x in_w\n",
    "        # g ~ bs x out_w\n",
    "        # Lg ~ bs x out_w\n",
    "        gp = orig_input\n",
    "        f = orig_output\n",
    "        Lf = output_gradient\n",
    "\n",
    "        # gp.T ~ in_w x bs\n",
    "        # Lf = bs x out_w\n",
    "        # \n",
    "        Lw = np.dot(gp.T, Lf)\n",
    "        self.grad_w = Lw\n",
    "        self.grad_b = np.sum(Lf.T, axis=1)\n",
    "\n",
    "        # Lf ~ bs x out_w\n",
    "        # w.T ~ out_w x in_w\n",
    "        Lgp = np.dot(Lf, self.w.T)\n",
    "        return Lgp\n",
    "\n",
    "    def descent(self, lr, l1_reg_lambda=0.0):\n",
    "        self.w -= np.sign(self.w) * l1_reg_lambda\n",
    "        self.w -= lr * self.grad_w\n",
    "        self.b -= lr * self.grad_b\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return [self.grad_w, self.grad_b]\n",
    "\n",
    "    def get_weigths(self):\n",
    "        return [self.w, self.b]\n",
    "\n",
    "class SepLogRegLoss(Layer):\n",
    "    def setup(self, exp_input):\n",
    "        self.exp_input = exp_input\n",
    "\n",
    "    def forward(self, input):\n",
    "        # g ~ bs x cls\n",
    "        # y ~ bs x cls\n",
    "        g = input\n",
    "        y = self.exp_input\n",
    "\n",
    "        sep_losses = -y*np.log(g) - (1.-y)*np.log(1.-g)\n",
    "        per_case_loss = np.sum(sep_losses, axis=1)\n",
    "        loss = np.average(per_case_loss)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, orig_input, orig_output, output_gradient):\n",
    "        bs = orig_input.shape[0]\n",
    "        y = self.exp_input\n",
    "        g = orig_input\n",
    "        return -1./bs * (y / g - (1.-y) / (1.-g))\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, input):\n",
    "        # input.shape = (batch_size, inp_width)\n",
    "        # w.shape = (inp_width, outp_width)\n",
    "        # b.shape = (outp_width)\n",
    "        return sigmoid(input)\n",
    "\n",
    "    def backward(self, orig_input, orig_output, output_gradient):\n",
    "        # gp ~ bs x in_w\n",
    "        # g ~ bs x out_w\n",
    "        # Lg ~ bs x out_w\n",
    "        g = orig_output\n",
    "        Lg = output_gradient\n",
    "        Lf = Lg * g * (1.0 - g)\n",
    "        return Lf\n",
    "\n",
    "class SepLogRegLoss(Layer):\n",
    "    def setup(self, exp_input):\n",
    "        self.exp_input = exp_input\n",
    "\n",
    "    def forward(self, input):\n",
    "        # g ~ bs x cls\n",
    "        # y ~ bs x cls\n",
    "        g = input\n",
    "        y = self.exp_input\n",
    "\n",
    "        sep_losses = -y*np.log(g) - (1.-y)*np.log(1.-g)\n",
    "        per_case_loss = np.sum(sep_losses, axis=1)\n",
    "        loss = np.average(per_case_loss)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, orig_input, orig_output, output_gradient):\n",
    "        bs = orig_input.shape[0]\n",
    "        y = self.exp_input\n",
    "        g = orig_input\n",
    "        return -1./bs * (y / g - (1.-y) / (1.-g))\n",
    "\n",
    "\n",
    "class Net(object):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward_backward(self, input):\n",
    "        inp = input\n",
    "        inouts = []\n",
    "        for l in self.layers:\n",
    "            out = l.forward(inp)\n",
    "            inouts.append((inp, out))\n",
    "            inp = out\n",
    "\n",
    "        grad = 1.\n",
    "        for l, (oinp, oout) in reversed(zip(self.layers, inouts)):\n",
    "            grad = l.backward(oinp, oout, grad)\n",
    "\n",
    "        return inouts\n",
    "\n",
    "    def grad_descent(self, lr):\n",
    "        for l in self.layers:\n",
    "            l.descent(lr, l1_reg_lambda = 0.1)\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.loss_layer = SepLogRegLoss()\n",
    "        \n",
    "        net = []\n",
    "        for ins, outs in zip(sizes[:-1], sizes[1:]):\n",
    "            net += [\n",
    "                Linear(ins, outs),\n",
    "                Sigmoid()\n",
    "            ]\n",
    "        net += [self.loss_layer]\n",
    "            \n",
    "        self.net = Net(net)\n",
    "        self.losses = []\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        return a\n",
    "\n",
    "    def run_minibach(self, mini_batch):\n",
    "        xs = mini_batch[0].reshape(-1, 784)\n",
    "        ys = mini_batch[1].reshape(-1, 10)\n",
    "        assert xs.shape[0] == ys.shape[0]\n",
    "        self.loss_layer.setup(ys)\n",
    "        return self.net.forward_backward(xs)\n",
    "\n",
    "    def numeric_gradient_check(self, mini_batch, eps=1e-6):\n",
    "        results = self.run_minibach(mini_batch)\n",
    "        base_loss = results[-1][1]\n",
    "\n",
    "        print 'Base loss', base_loss\n",
    "        print 'Changing single weigth'\n",
    "        l=1\n",
    "        wt=1\n",
    "        ind = 9\n",
    "        lw = self.net.layers[l].get_weigths()[wt]\n",
    "        grad_lw = self.net.layers[l].get_gradients()[wt]\n",
    "        org_grad = grad_lw.flat[ind]\n",
    "        orig = lw.flat[ind]\n",
    "        lw.flat[ind] += eps\n",
    "        results = self.run_minibach(mini_batch)\n",
    "        lw0_loss = results[-1][1]\n",
    "        print 'lw[ind] change loss', base_loss, 'delta', base_loss - lw0_loss\n",
    "        ngrad = (lw0_loss - base_loss) / eps\n",
    "        print 'lw[ind] grad', ngrad, 'analitical grad', org_grad\n",
    "        lw.flat[ind] = orig\n",
    "\n",
    "        results = self.run_minibach(mini_batch)\n",
    "        print 'After revert', results[-1][1]\n",
    "\n",
    "        fig, axs = plt.subplots(len(self.net.layers), 2, figsize=(10, 10))\n",
    "        diffs = []\n",
    "        for row, l in enumerate(self.net.layers):\n",
    "            for col, w in enumerate(l.get_gradients()):\n",
    "                for i in xrange(len(w.flat)):\n",
    "                    diffs.append(w.flat[i])\n",
    "                axs[row,col].hist(list(w.flat), 50)\n",
    "\n",
    "        num_diffs = []\n",
    "        for l in self.net.layers:\n",
    "            for w in l.get_weigths():\n",
    "                for i in xrange(len(w.flat)):\n",
    "                    orig = w.flat[i]\n",
    "                    w.flat[i] += eps\n",
    "                    results = self.run_minibach(mini_batch)\n",
    "                    new_loss = results[-1][1]\n",
    "                    num_diff = (new_loss - base_loss) / eps\n",
    "                    num_diffs.append(num_diff)\n",
    "                    w.flat[i] = orig\n",
    "\n",
    "        num_diffs = np.array(num_diffs)\n",
    "        diffs = np.array(diffs)\n",
    "        error = np.abs(num_diffs-diffs)\n",
    "        print np.min(error), np.average(error), np.max(error)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, lr):\n",
    "        outputs = self.run_minibach(mini_batch)\n",
    "        self.losses.append(outputs[-1][1])\n",
    "        self.net.grad_descent(lr)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        # Count the number of correct answers for test_data\n",
    "        outputs = self.run_minibach(test_data)\n",
    "        net_output = outputs[-1][0]\n",
    "        test_results = [(np.argmax(net_output[i]),\n",
    "                         np.argmax(test_data[1][i]))\n",
    "                        for i in range(len(test_data[0]))]\n",
    "        #print test_results\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, lr, test_data=None):\n",
    "        self.losses = []\n",
    "        train_size = training_data.images.shape[0]\n",
    "        if test_data:\n",
    "            test_size = test_data.images.shape[0]\n",
    "        for j in xrange(epochs):\n",
    "            for k in range(train_size/mini_batch_size):\n",
    "                self.update_mini_batch(training_data.next_batch(mini_batch_size), lr)\n",
    "            if test_data and j % 10 == 0:\n",
    "                res = np.mean([self.evaluate(test_data.next_batch(mini_batch_size)) for k in range(test_size/mini_batch_size)])/mini_batch_size\n",
    "                print \"Epoch {0}: {1}\".format(j, res),\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j),\n",
    "        plt.plot(self.losses)\n",
    "\n",
    "\n",
    "network = Network([784,30,10])\n",
    "# network.numeric_gradient_check(mnist.train.next_batch(50))\n",
    "network.SGD(mnist.train,epochs=11,mini_batch_size=200,lr=0.01,test_data=mnist.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}