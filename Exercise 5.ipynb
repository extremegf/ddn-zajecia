{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Softmax regression\n",
    "\n",
    "In this exercise you will train a softmax regression model to recognize handwritten digits.\n",
    "  \n",
    "The general setup is as follows:\n",
    "* we are given a set of pairs $(x, y)$, where $x \\in R^D$ is a vector of real numbers representing the features, and $y \\in \\{1,...,c\\}$ is the target (in our case we have ten classes, so $c=10$),\n",
    "* for a given $x$ we model the probability of $y=j$ by $$h(x)_j=p_j = \\frac{e^{w_j^Tx}}{\\sum_{i=1}^c e^{w_i^Tx}},$$\n",
    "* to find the right $w$ we will optimize the so called multiclass log loss:\n",
    "$$L(y,p) = \\log{p_y},$$\n",
    "$$J(w) = -\\frac{1}{n}\\sum_{i=1}^n L(y_i,h(x)),$$\n",
    "* with the loss function in hand we can improve our guesses iteratively:\n",
    "    * $w_{ij}^{t+1} = w_{ij}^t - \\text{step_size} \\cdot \\frac{\\partial J(w)}{\\partial w_{ij}}$,\n",
    "* we can end the process after some predefined number of epochs (or when the changes are no longer meaningful)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's start with importing the MNIST dataset. For convenience, let's use Google's script from TensorFlow tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!wget -O MNIST_data.zip https://www.dropbox.com/sh/z7h50270eckbrd3/AAAmBulcP1UaEYBYyvBKqXSwa?dl=1\n",
    "!unzip MNIST_data.zip -d MNIST_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# needs tensorflow 1.0\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "train = mnist.train.next_batch(1000)\n",
    "test = mnist.train.next_batch(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's take a look at the data. Both train and test are tuples with two numpy array. In the first array you'll find the images (encoded as pixel intensities) and in the second one you'll find the labels (one-hot encoded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print type(train)\n",
    "\n",
    "print type(train[0])\n",
    "print type(train[1])\n",
    "\n",
    "print (train[0][4] > 0.01).reshape(28,28).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let us see the data in a more humane way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "samples = range(num_samples)\n",
    "fig, subplots = plt.subplots(1, num_samples)\n",
    "fig.set_size_inches(15, 15)\n",
    "\n",
    "for i, s in enumerate(subplots.flatten()):\n",
    "    s.imshow(np.reshape(train[0][i, :], [28, 28]), cmap='gray')\n",
    "    s.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we prepare $X$ and $y$ variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = train[0]\n",
    "y = train[1]\n",
    "\n",
    "X_test = test[0]\n",
    "y_test = test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To train the model we will (obviously) use gradient descent. Inside the loop we need a method to compute the gradients. Let's start with implementing it, together with some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We will store the weights in a D x c matrix, where D is the number of features, and c is the number of classes\n",
    "#weights = (...) # TODO: Fill in, be sure to have the right shape!\n",
    "weights = np.zeros([X.shape[1], 10])\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    # z is cases x outs\n",
    "    up = np.exp(z)\n",
    "    down = np.sum(up, axis=1).reshape(-1, 1)\n",
    "    return up / down\n",
    "\n",
    "def almost_equal(a, b):\n",
    "    return a - b < 1e9\n",
    "\n",
    "almost_equal(softmax(np.log(np.array([[1, 2], [3, 4]]))),\n",
    "             np.array([[1./3, 2./3], [3./7, 4./7]]))\n",
    "\n",
    "\n",
    "def predict(weights, X):\n",
    "    return softmax(np.matmul(X, weights))\n",
    "\n",
    "def compute_loss_and_gradients(weights, X, y, l2_reg):\n",
    "    p = predict(weights, X)\n",
    "    masked_loss = -np.log(p) * y\n",
    "    assert sum((masked_loss != 0.).flatten()) == X.shape[0]\n",
    "    loss_per_example = np.sum(masked_loss, axis=1)\n",
    "    loss = np.average(loss_per_example) + l2_reg * np.sum(weights ** 2)\n",
    "    \n",
    "    # p shape [example x class prob]\n",
    "    # y shape [example x class exp. prob]\n",
    "    # X shape [example x features]\n",
    "    grad = np.matmul((p - y).T,X).T / X.shape[0] + l2_reg * 2 * weights\n",
    "    \n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are now in position to complete the training pipeline.\n",
    "\n",
    "If you have problems with convergence, be sure to check the gradients numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l2_reg = 0.5\n",
    "n_epochs = 25\n",
    "lr = 0.05\n",
    "\n",
    "losses = []\n",
    "for i in range(n_epochs):\n",
    "    loss, grad = compute_loss_and_gradients(weights, X, y, l2_reg)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    weights -= lr * grad\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now compute your accuracy on the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conf_matrix(pred_class, real_class, class_no):\n",
    "    print 'Row represents real classes - similar to P(y|yreal=row)'\n",
    "    for real_cl in range(class_no):\n",
    "        row = []\n",
    "        for pred_cl in range(class_no):\n",
    "            cnt = 0\n",
    "            for p, r in zip(pred_class, real_class):\n",
    "                if p == pred_cl and r == real_cl:\n",
    "                    cnt += 1\n",
    "            row.append(cnt)\n",
    "        \n",
    "        print real_cl,\n",
    "        row = np.array(row)\n",
    "        for v in row * 1000 / np.sum(row):\n",
    "            print '%03d' % int(v),\n",
    "        print ''\n",
    "        \n",
    "def check_on_set(X, y, weights):\n",
    "    p = predict(weights, X)\n",
    "    pred_class, real_class = np.argmax(p, axis=1), np.argmax(y, axis=1)\n",
    "    print 'Accuracy:', np.average(pred_class == real_class)\n",
    "    conf_matrix(pred_class, real_class, 10)\n",
    "    \n",
    "check_on_set(X, y, weights)\n",
    "check_on_set(X_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also visualize the weights learned by our algorithm. Try to anticipate the result before executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_w(weights):\n",
    "    fig, subplots = plt.subplots(1, 10)\n",
    "    fig.set_size_inches(10, 10)\n",
    "\n",
    "    for i, s in enumerate(subplots.flatten()):\n",
    "        s.imshow(np.reshape(np.array(weights[:, i]), [28, 28]), cmap='gray')\n",
    "        s.axis('off')\n",
    "        \n",
    "plot_w(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that we only used a small portion of the data to develop the model. Now, implement the training on full data. \n",
    "Make sure to leverage the `mnist.train.next_batch(...)` method. Also, validate your model properly and find a good value for `l2_reg` hyperparameter. Try to experiment with `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_val, y_val = mnist.validation.next_batch(2000)\n",
    "\n",
    "def train_mnist(l2_reg = 0.0, n_epochs = 300, lr = 0.45, batch_sz = 50):\n",
    "    weights = np.zeros([28**2, 10])\n",
    "    losses = []\n",
    "    for i in range(n_epochs):\n",
    "        batch = mnist.train.next_batch(batch_sz)\n",
    "        loss, grad = compute_loss_and_gradients(weights, batch[0], batch[1], l2_reg)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        weights -= lr * grad\n",
    "        \n",
    "    p = predict(weights, X_val)\n",
    "    pred_class, real_class = np.argmax(p, axis=1), np.argmax(y_val, axis=1)\n",
    "    return weights, np.average(pred_class == real_class)\n",
    " \n",
    "par_val, accs = [], []\n",
    "for par in np.exp(np.linspace(np.log(0.3), np.log(0.7), 20)):\n",
    "    par_val.append(par)\n",
    "    weights, acc = train_mnist(lr=par)\n",
    "    accs.append(acc)\n",
    "plt.scatter(par_val, accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights, val_acc = train_mnist()\n",
    "print 'valacc', val_acc\n",
    "X_test, y_test = mnist.test.next_batch(10000)\n",
    "check_on_set(X_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
