{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this exercise your task is to fill in the gaps in this code by implementing the backpropagation algorithm Once this is done, you can run the network on the MNIST example and see how it performs. Feel free to play with the parameters.\n",
    "If you found this task too easy, try to implement a \"fully vectorized\" version, i.e. one using matrix operations instead of going over examples one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7325a0e3ca42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;31m# network.numeric_gradient_check(mnist.train.next_batch(50))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# apply regularization\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    # Derivative of the sigmoid\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class Layer(object):\n",
    "    def descent(self, lr):\n",
    "        pass\n",
    "    \n",
    "    def get_weigths(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        return []\n",
    "\n",
    "class FC(Layer):\n",
    "    def __init__(self, ins, outs):\n",
    "        self.w = np.random.normal(scale=(1.0  / (ins + outs)), size=[ins, outs])\n",
    "        self.b = np.random.normal(scale=0.1, size=[outs])\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input.shape = (batch_size, inp_width)\n",
    "        # w.shape = (inp_width, outp_width)\n",
    "        # b.shape = (outp_width)\n",
    "        return sigmoid(np.dot(input, self.w) + self.b)\n",
    "    \n",
    "    def backward(self, orig_input, orig_output, output_gradient):\n",
    "        # gp ~ bs x in_w\n",
    "        # g ~ bs x out_w\n",
    "        # Lg ~ bs x out_w\n",
    "        gp = orig_input\n",
    "        g = orig_output\n",
    "        Lg = output_gradient\n",
    "        Lf = Lg * g * (1.0 - g)\n",
    "\n",
    "        # gp.T ~ in_w x bs\n",
    "        # Lf = bs x out_w\n",
    "        # \n",
    "        Lw = np.dot(gp.T, Lf)\n",
    "        self.grad_w = Lw\n",
    "        self.grad_b = np.sum(Lf.T, axis=1)\n",
    "\n",
    "        # Lf ~ bs x out_w\n",
    "        # w.T ~ out_w x in_w\n",
    "        Lgp = np.dot(Lf, self.w.T)\n",
    "        return Lgp\n",
    "    \n",
    "    def descent(self, lr):\n",
    "        self.w -= lr * self.grad_w\n",
    "        self.b -= lr * self.grad_b\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return [self.grad_w, self.grad_b]\n",
    "\n",
    "    def get_weigths(self):\n",
    "        return [self.w, self.b]\n",
    "        \n",
    "class SepLogRegLoss(Layer):\n",
    "    def setup(self, exp_input):\n",
    "        self.exp_input = exp_input\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # g ~ bs x cls\n",
    "        # y ~ bs x cls\n",
    "        g = input\n",
    "        y = self.exp_input\n",
    "        \n",
    "        sep_losses = -y*np.log(g) - (1.-y)*np.log(1.-g)\n",
    "        per_case_loss = np.sum(sep_losses, axis=1)\n",
    "        loss = np.average(per_case_loss)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, orig_input, orig_output, output_gradient):\n",
    "        bs = orig_input.shape[0]\n",
    "        y = self.exp_input\n",
    "        g = orig_input\n",
    "        return -1./bs * (y / g - (1.-y) / (1.-g))\n",
    "    \n",
    "    \n",
    "class Net(object):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward_backward(self, input):\n",
    "        inp = input\n",
    "        inouts = []\n",
    "        for l in self.layers:\n",
    "            out = l.forward(inp)\n",
    "            inouts.append((inp, out))\n",
    "            inp = out\n",
    "            \n",
    "        grad = 1.\n",
    "        for l, (oinp, oout) in reversed(zip(self.layers, inouts)):\n",
    "            grad = l.backward(oinp, oout, grad)\n",
    "            \n",
    "        return inouts\n",
    "    \n",
    "    def grad_descent(self, lr):\n",
    "        for l in self.layers:\n",
    "            l.descent(lr)\n",
    "    \n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.loss_layer = SepLogRegLoss()\n",
    "        self.net = Net(\n",
    "            [FC(ins, outs) for ins, outs in zip(sizes[:-1], sizes[1:])] +\n",
    "            [self.loss_layer]\n",
    "        )\n",
    "        self.losses = []\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        return a\n",
    "    \n",
    "    def run_minibach(self, mini_batch):\n",
    "        xs = mini_batch[0].reshape(-1, 784)\n",
    "        ys = mini_batch[1].reshape(-1, 10)\n",
    "        assert xs.shape[0] == ys.shape[0]\n",
    "        self.loss_layer.setup(ys)\n",
    "        return self.net.forward_backward(xs)\n",
    "    \n",
    "    def numeric_gradient_check(self, mini_batch, eps=1e-6):\n",
    "        results = self.run_minibach(mini_batch)\n",
    "        base_loss = results[-1][1]\n",
    "        \n",
    "        print 'Base loss', base_loss\n",
    "        print 'Changing single weigth'\n",
    "        l=1\n",
    "        wt=1\n",
    "        ind = 9\n",
    "        lw = self.net.layers[l].get_weigths()[wt]\n",
    "        grad_lw = self.net.layers[l].get_gradients()[wt]\n",
    "        org_grad = grad_lw.flat[ind]\n",
    "        orig = lw.flat[ind]\n",
    "        lw.flat[ind] += eps\n",
    "        results = self.run_minibach(mini_batch)\n",
    "        lw0_loss = results[-1][1]\n",
    "        print 'lw[ind] change loss', base_loss, 'delta', base_loss - lw0_loss \n",
    "        ngrad = (lw0_loss - base_loss) / eps\n",
    "        print 'lw[ind] grad', ngrad, 'analitical grad', org_grad\n",
    "        lw.flat[ind] = orig\n",
    "\n",
    "        results = self.run_minibach(mini_batch)\n",
    "        print 'After revert', results[-1][1]\n",
    "        \n",
    "        fig, axs = plt.subplots(len(self.net.layers), 2, figsize=(10, 10))\n",
    "        diffs = []\n",
    "        for row, l in enumerate(self.net.layers):\n",
    "            for col, w in enumerate(l.get_gradients()):\n",
    "                for i in xrange(len(w.flat)):\n",
    "                    diffs.append(w.flat[i])\n",
    "                axs[row,col].hist(list(w.flat), 50)\n",
    "        \n",
    "        num_diffs = []\n",
    "        for l in self.net.layers:\n",
    "            for w in l.get_weigths():\n",
    "                for i in xrange(len(w.flat)):\n",
    "                    orig = w.flat[i]\n",
    "                    w.flat[i] += eps\n",
    "                    results = self.run_minibach(mini_batch)\n",
    "                    new_loss = results[-1][1]\n",
    "                    num_diff = (new_loss - base_loss) / eps\n",
    "                    num_diffs.append(num_diff)\n",
    "                    w.flat[i] = orig\n",
    "\n",
    "        num_diffs = np.array(num_diffs)\n",
    "        diffs = np.array(diffs)\n",
    "        error = np.abs(num_diffs-diffs)\n",
    "        print np.min(error), np.average(error), np.max(error)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, lr):\n",
    "        outputs = self.run_minibach(mini_batch)\n",
    "        self.losses.append(outputs[-1][1])\n",
    "        self.net.grad_descent(lr)\n",
    "        \n",
    "    def evaluate(self, test_data):\n",
    "        # Count the number of correct answers for test_data\n",
    "        outputs = self.run_minibach(test_data)\n",
    "        net_output = outputs[-1][0]\n",
    "        test_results = [(np.argmax(net_output[i]), \n",
    "                         np.argmax(test_data[1][i]))\n",
    "                        for i in range(len(test_data[0]))]\n",
    "        #print test_results\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, lr, test_data=None):\n",
    "        self.losses = []\n",
    "        train_size = training_data.images.shape[0]\n",
    "        if test_data:\n",
    "            test_size = test_data.images.shape[0]\n",
    "        for j in xrange(epochs):\n",
    "            for k in range(train_size/mini_batch_size):\n",
    "                self.update_mini_batch(training_data.next_batch(mini_batch_size), lr)\n",
    "            if test_data and j % 10 == 0:\n",
    "                res = np.mean([self.evaluate(test_data.next_batch(mini_batch_size)) for k in range(test_size/mini_batch_size)])/mini_batch_size\n",
    "                print \"Epoch {0}: {1}\".format(j, res),\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j),\n",
    "        plt.plot(self.losses)\n",
    "\n",
    "\n",
    "network = Network([784,30,10])\n",
    "# network.numeric_gradient_check(mnist.train.next_batch(50))\n",
    "network.SGD(mnist.train,epochs=1,mini_batch_size=200,lr=0.01,test_data=mnist.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}